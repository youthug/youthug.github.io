<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="HDFS 高可用（HA）集群搭建记录"><meta name="keywords" content="Hadoop,HDFS,HDFS HA"><meta name="author" content="Yout"><meta name="copyright" content="Yout"><title>HDFS 高可用（HA）集群搭建记录 | 可爱迷人的反派</title><link rel="shortcut icon" href="http://pouotmqe7.bkt.clouddn.com/b/i/site/favicon-16x16.png"><link rel="stylesheet" href="/blog/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/blog/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#无关紧要的配置"><span class="toc-number">1.</span> <span class="toc-text">无关紧要的配置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#系统基础配置"><span class="toc-number">2.</span> <span class="toc-text">系统基础配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#网络"><span class="toc-number">2.1.</span> <span class="toc-text">网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#主机名和域名映射"><span class="toc-number">2.2.</span> <span class="toc-text">主机名和域名映射</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSH-免密验证"><span class="toc-number">2.3.</span> <span class="toc-text">SSH 免密验证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop-用户"><span class="toc-number">2.4.</span> <span class="toc-text">* Hadoop 用户</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#安装-JDK"><span class="toc-number">3.</span> <span class="toc-text">安装 JDK</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#从主机复制压缩包至虚拟机"><span class="toc-number">3.1.</span> <span class="toc-text">从主机复制压缩包至虚拟机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#解压压缩包"><span class="toc-number">3.2.</span> <span class="toc-text">解压压缩包</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#配置-PATH"><span class="toc-number">3.3.</span> <span class="toc-text">配置 $PATH</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#验证-JDK-是否成功安装"><span class="toc-number">3.4.</span> <span class="toc-text">验证 JDK 是否成功安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分发文件"><span class="toc-number">3.5.</span> <span class="toc-text">分发文件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#上传-Hadoop，Zookeeper-压缩包"><span class="toc-number">4.</span> <span class="toc-text">上传 Hadoop，Zookeeper 压缩包</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#上传文件至虚拟机"><span class="toc-number">4.1.</span> <span class="toc-text">上传文件至虚拟机</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#配置-Hadoop"><span class="toc-number">5.</span> <span class="toc-text">配置 Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS"><span class="toc-number">5.1.</span> <span class="toc-text">HDFS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MapReduce"><span class="toc-number">5.2.</span> <span class="toc-text">MapReduce</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Yarn"><span class="toc-number">5.3.</span> <span class="toc-text">Yarn</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Zookeeper"><span class="toc-number">5.4.</span> <span class="toc-text">Zookeeper</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#发送文件"><span class="toc-number">6.</span> <span class="toc-text">发送文件</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#集群启动"><span class="toc-number">7.</span> <span class="toc-text">集群启动</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#遇到的问题"><span class="toc-number">8.</span> <span class="toc-text">遇到的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Zookeeper-启动了，查看状态却提示可能未启动"><span class="toc-number">8.1.</span> <span class="toc-text">Zookeeper 启动了，查看状态却提示可能未启动</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#记录"><span class="toc-number">9.</span> <span class="toc-text">记录</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#备用"><span class="toc-number">10.</span> <span class="toc-text">备用</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://pouotmqe7.bkt.clouddn.com/b/i/site/avatar.png"></div><div class="author-info__name text-center">Yout</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/blog/archives"><span class="pull-left">Articles</span><span class="pull-right">4</span></a><a class="author-info-articles__tags article-meta" href="/blog/tags"><span class="pull-left">Tags</span><span class="pull-right">5</span></a><a class="author-info-articles__categories article-meta" href="/blog/categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://pouotmqe7.bkt.clouddn.com/8700af19ly1fvpabr30o6j22yo1o0q7i.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/blog/">可爱迷人的反派</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">主页</a><a class="site-page" href="/blog/archives">文章</a><a class="site-page" href="/blog/tags">标签</a><a class="site-page" href="/blog/categories">分类</a></span></div><div id="post-info"><div id="post-title">HDFS 高可用（HA）集群搭建记录</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-03-30</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/blog/categories/笔记/">笔记</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/blog/categories/笔记/大数据/">大数据</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>什么是 HA ?<br>  <strong>High Available : 高可用</strong><br>  虽然 HDFS 存在多个副本，但 NameNode 可能会出现单节点故障。对于只有一个 NameNode 节点的集群，一旦该节点出现故障，集群将无法使用直至重新启动。<br>  通过开启 HDFS 的 HA 功能，通过在不同节点上设置 Active/Standby 多个 NameNode，当 Active NameNode 出现故障时，可以很快的将 Standby NameNode 切换至 Active 状态。只有 Active NameNode 才能对外提供读写服务。</p>
<p>环境：</p>
<ul>
<li>CentOS 7.6<font color="#D6D5B7">.1810</font> Minimal</li>
<li>NAT 网络模式（虚拟机）</li>
<li>JDK 1.8</li>
<li>Hadoop 3.2.0</li>
<li>Zookeeper 3.4.13</li>
</ul>
<p>集群规划（3 台）:</p>
<table>
<thead>
<tr>
<th style="text-align:center">主机名</th>
<th style="text-align:center">NameNode</th>
<th style="text-align:center">DataNode</th>
<th style="text-align:center">ResourceManager</th>
<th style="text-align:center">NodeManager</th>
<th style="text-align:center">Zookeeper</th>
<th style="text-align:center">JournalNode</th>
<th style="text-align:center">ZKFC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">master</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">master2</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
<tr>
<td style="text-align:center">slave1</td>
<td style="text-align:center"></td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
<td style="text-align:center">√</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<h1 id="无关紧要的配置"><a href="#无关紧要的配置" class="headerlink" title="无关紧要的配置"></a>无关紧要的配置</h1><ul>
<li>CentOS 7 安装完后感觉分辨率太高，小屏幕顶不住，修改分辨率<br><code>vi /boot/grub2/grub.cfg</code>（CentOS 7）<br>调整为 800x600x32 的分辨率，在 <code>linux16 /vmlinuz-x.xx.x</code> 行，末尾添加：<code>vga=0x340</code>，重启生效。修改为 <code>vga=ask</code> 将在启动时提示选择显示模式。<br><font color="#F4606C"><strong>勿改 <code>linux16 /vmlinuz-0-rescue</code> 行</strong></font><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190330111734517.png.ylit" alt><br>可用显示模式：<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190330110837739.png.ylit" alt></li>
</ul>
<h1 id="系统基础配置"><a href="#系统基础配置" class="headerlink" title="系统基础配置"></a>系统基础配置</h1><p>由于是最小化安装（Minimal），部分用得到的命令可能需要手动安装。</p>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>分配固定 IP。</p>
<table>
<thead>
<tr>
<th style="text-align:center">主机名</th>
<th style="text-align:center">IP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">master</td>
<td style="text-align:center">192.168.222.128</td>
</tr>
<tr>
<td style="text-align:center">master2</td>
<td style="text-align:center">192.168.222.129</td>
</tr>
<tr>
<td style="text-align:center">slave1</td>
<td style="text-align:center">192.168.222.130</td>
</tr>
</tbody>
</table>
<ol>
<li>启用网卡<br>系统安装完后网卡默认不自启。<ul>
<li>查看设备：<code>ip addr</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190331011328704.png.ylit" alt></li>
<li>启用：<code>ifup ens33</code><br>默认为 dhcp 自动获取 IP。<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190331011607263.png.ylit" alt></li>
</ul>
</li>
<li>安装 net-tools<br>基本网络实用程序集合。含常用的 <code>ifconfig</code>，<code>netstat</code> 命令等，方便查看和配置网络。<br><code>yum install -y net-tools</code></li>
<li><p>设置网卡自动启动，分配固定 IP<br>直接修改配置文件：<br><code>vi /etc/sysconfig/network-scripts/ifcfg-ens33</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">BOOTPROTO=static # 使用什么协议，static（静态）</span><br><span class="line">...</span><br><span class="line">ONBOOT=yes # 开机自启</span><br><span class="line"></span><br><span class="line"># IP 和网关等信息</span><br><span class="line">IPADDR=192.168.222.128</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.168.222.2</span><br><span class="line">DNS1=8.8.8.8</span><br><span class="line">DNS2=4.4.4.4</span><br></pre></td></tr></table></figure>
<p>图中的 <code>BOOTPROTO</code> 应改为 <code>static</code>，不然手动配置的固定 IP 可能不生效。<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190331115315873.png.ylit" alt="master 的网络配置"></p>
</li>
</ol>
<h2 id="主机名和域名映射"><a href="#主机名和域名映射" class="headerlink" title="主机名和域名映射"></a>主机名和域名映射</h2><ul>
<li>修改主机名<br>三台分别设置为 master master2 slave1<br>如：<code>hostnamectl set-hostname master</code>（使用此命令修改无需重启就可永久生效）</li>
<li><p>修改 hosts 文件<br><font color="#F4606C">这里遇到过一个问题，已解决：Zookeeper 启动了，查看状态却提示可能未启动</font><br><code>vi /etc/hosts</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.222.128 master</span><br><span class="line">192.168.222.129 master2</span><br><span class="line">192.168.222.130 slave1</span><br></pre></td></tr></table></figure>
<p><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401080446747.png.ylit" alt></p>
</li>
</ul>
<h2 id="SSH-免密验证"><a href="#SSH-免密验证" class="headerlink" title="SSH 免密验证"></a>SSH 免密验证</h2><p>所有节点分别执行：</p>
<ul>
<li>生成密钥对<br><code>ssh-keygen -t rsa</code></li>
<li>拷贝公钥<br><code>ssh-copy-id master</code><br><code>ssh-copy-id master2</code><br><code>ssh-copy-id slave1</code></li>
</ul>
<h2 id="Hadoop-用户"><a href="#Hadoop-用户" class="headerlink" title="* Hadoop 用户"></a>* Hadoop 用户</h2><p>* 将操作 Hadoop 的用户独立出来，可能会更易于管理和维护，但可能会引出一系列权限问题。可选。（root 用户下操作，3 台都要创建）</p>
<ul>
<li>创建 hadoop 用户组<br><code>groupadd hadoop</code></li>
<li>创建 hadoop 用户，并加入 hadoop 用户组<br><code>useradd -g hadoop hadoop</code></li>
<li>修改 hadoop 用户的密码<br><code>passwd hadoop</code></li>
<li>将 <code>/usr/local/src</code> 目录所属组改为 hadoop 组<br><code>chgrp /usr/local/src hadoop</code><br>* 如需更改其下子目录和文件需加上 <code>-r</code> 参数</li>
<li>为 <code>/usr/local/src</code> 目录的所属组赋予写权限<br><code>chmod g+w /usr/local/src</code><br>* 如需更改其下子目录和文件需加上 <code>-r</code> 参数</li>
</ul>
<h1 id="安装-JDK"><a href="#安装-JDK" class="headerlink" title="安装 JDK"></a>安装 JDK</h1><ul>
<li>JDK 最好使用 1.8 版本，之后的版本有变动，需额外设置才能成功启动 HDFS</li>
</ul>
<h2 id="从主机复制压缩包至虚拟机"><a href="#从主机复制压缩包至虚拟机" class="headerlink" title="从主机复制压缩包至虚拟机"></a>从主机复制压缩包至虚拟机</h2><p>用 SecureCRT 的 SFTP 上传，Alt + P 呼出，拖入文件开始发送。<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402023118935.png.ylit" alt></p>
<h2 id="解压压缩包"><a href="#解压压缩包" class="headerlink" title="解压压缩包"></a>解压压缩包</h2><p>把 JDK 文件解压至 <code>/usr/local/src</code> 下：<br><code>tar -xzvf ~/jdk-8u201-linux-x64.tar.gz -C /usr/local/src</code>。</p>
<h2 id="配置-PATH"><a href="#配置-PATH" class="headerlink" title="配置 $PATH"></a>配置 $PATH</h2><ul>
<li><p>在 <code>/etc/profile</code> 中添加：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Java</span><br><span class="line">export JAVA_HOME=/usr/local/src/jdk1.8.0_201</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新登入或使用 source 让改动生效<br><code>source /etc/profile</code></p>
</li>
</ul>
<h2 id="验证-JDK-是否成功安装"><a href="#验证-JDK-是否成功安装" class="headerlink" title="验证 JDK 是否成功安装"></a>验证 JDK 是否成功安装</h2><p><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402022325690.png.ylit" alt></p>
<h2 id="分发文件"><a href="#分发文件" class="headerlink" title="分发文件"></a>分发文件</h2><ul>
<li>/usr/local/src<br>master2：<code>scp -r /usr/local/src master2:/usr/local</code><br>slave1：<code>scp -r /usr/local/src slave1:/usr/local</code></li>
<li>/etc/profile 和 /etc/hosts<br><code>scp /etc/profile master2:/etc</code><br><code>scp /etc/hosts master2:/etc</code><br>同样发送到 slave1 上。</li>
</ul>
<hr>
<p>下面的步骤将使用 hadoop 用户进行操作<br>新开一个连接，登入 hadoop 用户：<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402022835034.png.ylit" alt><br>为 3 个节点的 hadoop 用户做好 SSH 免密验证。</p>
<hr>
<h1 id="上传-Hadoop，Zookeeper-压缩包"><a href="#上传-Hadoop，Zookeeper-压缩包" class="headerlink" title="上传 Hadoop，Zookeeper 压缩包"></a>上传 Hadoop，Zookeeper 压缩包</h1><p>也可使用 <code>wget</code> 直接在虚拟机中下载：</p>
<ul>
<li>Hadoop 2.9.2（清华源）：<code>wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz</code></li>
<li>Zookeeper 3.4.14：<img src="https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/" alt="清华源"></li>
</ul>
<h2 id="上传文件至虚拟机"><a href="#上传文件至虚拟机" class="headerlink" title="上传文件至虚拟机"></a>上传文件至虚拟机</h2><p>SFTP 上传：<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402023358788.png.ylit" alt></p>
<ul>
<li><p>修改 hadoop 用户的配置文件 <code>/home/hadoop/.bashrc</code>（或直接在 <code>/etc/profile</code> 中一并设置），添加内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># -- HADOOP ENVIRONMENT VARIABLES START -- #</span><br><span class="line">## Hadoop -v3.2.0</span><br><span class="line">export HADOOP_HOME=/usr/local/src/hadoop-3.2.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br><span class="line"></span><br><span class="line"># Zookeeper -v3.4.13</span><br><span class="line">export ZK_HOME=/usr/local/src/zookeeper-3.4.13</span><br><span class="line">export PATH=$PATH:$ZK_HOME/bin</span><br><span class="line"># -- HADOOP ENVIRONMENT VARIABLES FINISH -- #</span><br></pre></td></tr></table></figure>
</li>
<li><p>重新登入或 <code>source ~/.bashrc</code> 生效</p>
</li>
</ul>
<h1 id="配置-Hadoop"><a href="#配置-Hadoop" class="headerlink" title="配置 Hadoop"></a>配置 Hadoop</h1><p>Hadoop 的 6 个配置文件（$HADOOP_HOME/etc/hadoop）：</p>
<table>
<thead>
<tr>
<th style="text-align:center">组件</th>
<th style="text-align:center">配置文件</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">HDFS</td>
<td style="text-align:center">hadoop-env.sh, core-site.xml, hdfs-site.xml, workers</td>
</tr>
<tr>
<td style="text-align:center">MapReduce</td>
<td style="text-align:center">mapred-site.xml</td>
</tr>
<tr>
<td style="text-align:center">Yarn</td>
<td style="text-align:center">yarn-site.xml</td>
</tr>
</tbody>
</table>
<p>Zookeeper 的配置文件：<code>$ZK_HOME/conf/zoo.cfg</code></p>
<p>参考自：</p>
<blockquote>
<ul>
<li><a href="https://zm8.sm-tc.cn/?src=l4uLj4zF0NCIiIjRnJGdk5CYjNGckJLQk4qXnpCVlprQno2LlpyTmozQxs3MycfMxtGXi5KT&amp;uid=e396ee26f8cf201a0a0967f9e860af7e&amp;hid=a1a1c9eb6350d05ae25d6113a6fb4e28&amp;pos=1&amp;cid=9&amp;time=1553592973441&amp;from=click&amp;restype=1&amp;pagetype=0020000002000408&amp;bu=ss_doc&amp;query=hadoopha%E4%B8%89%E5%8F%B0&amp;mode=&amp;v=1&amp;force=true&amp;wap=false&amp;uc_param_str=dnntnwvepffrgibijbprsvdsdichei" target="_blank" rel="noopener">Hadoop集群搭建：用三台云服务器搭建HA集群（过程记录和分享）</a></li>
<li>官方文档：<a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html" target="_blank" rel="noopener">HDFS High Availability Using the Quorum Journal Manager</a></li>
<li><a href="https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/" target="_blank" rel="noopener">How to Set Up Hadoop Cluster with HDFS High Availability</a></li>
</ul>
</blockquote>
<p>切换至配置文件目录：<code>cd $HADOOP_HOME/etc/hadoop</code></p>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><ol>
<li><p>hadoop-env.sh<br>取消注释（删掉前面的 <code>#</code>） JAVA_HOME 和 HADOOP_HOME，填入相应路径<br><font color="#F4606C"><strong>至少要指定 JAVA_HOME</strong></font></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># The java implementation to use. By default, this environment</span><br><span class="line"># variable is REQUIRED on ALL platforms except OS X!</span><br><span class="line">export JAVA_HOME=/usr/local/src/jdk1.8.0_201</span><br><span class="line"></span><br><span class="line"># Location of Hadoop.  By default, Hadoop will attempt to determine</span><br><span class="line"># this location based upon its execution path.</span><br><span class="line">export HADOOP_HOME=/usr/local/src/hadoop-3.2.0</span><br></pre></td></tr></table></figure>
<p><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190331025547767.png.ylit" alt></p>
</li>
<li><p>core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- HDFS的NameService，任意 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://ha-cluster&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- Hadoop存放元数据文件的目录 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/usr/local/src/hadoop-3.2.0/tmp&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 流文件的缓冲区大小，单位：KB --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.file.buffer.size&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定Zookeeper集群的地址以进行故障自动转移 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:2181,master2:2181,slave1:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/HA/data/journalnode&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定HDFS的NameServices，需和core-site.xml中保持一致 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;ha-cluster&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定ha-cluster下的NameNodes（任取） --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.ha-cluster&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- NameNodes的rpc通信地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.ha-cluster.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.ha-cluster.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master2:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- NameNodes的http通信地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.ha-cluster.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:9870&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.ha-cluster.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master2:9870&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;qjournal://master:8485;master2:8485;slave1:8485/ha-cluster&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- HDFS客户端用于联系Active NameNode的Java类，也用于故障转移实现 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.ha-cluster&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 脚本或Java类的列表，用于在故障转移期间屏蔽Active NameNode，多个方法使用换行进行分隔 --&gt;</span><br><span class="line">  &lt;!-- sshfence可用于SSH到Active NameNode并终止进程 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定SSH密钥文件列表，逗号分隔。sshfence需要免密验证以登录至其他NameNode节点 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 可选，配置使用非标准用户或端口来执行SSH --&gt;</span><br><span class="line">  &lt;!--&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;sshfence(hadoop:22)&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;--&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 可选，配置SSH超时时间 单位：毫秒 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;30000&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- JournalNode守护程序用于存储其本地状态的路径 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;/home/hadoop/HA/data/jn_local&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 开启NameNode故障自动切换 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 设置副本数为2 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>workers（原 slaves）<br><code>$HADOOP_HOME/sbin</code> 中的脚本以及 <code>hdfs</code> 可通过文件中列出的主机名去启动节点上对应的进程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">master2</span><br><span class="line">slave1</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><ul>
<li>mapred-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定mr框架为yarn --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- map任务内存大小，默认1G --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;230&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- reduce任务内存大小，默认1G --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;460&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- map任务运行的JVM进程内存大小，默认-Xmx200M --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx184m&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- reduce任务运行的JVM进程内存大小，默认-Xmx200M --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx368m&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- MR AppMaster运行内存，默认1536M --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;460&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- MR AppMaster运行的JVM进程内存，默认-Xmx1024m --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;-Xmx368m&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><ul>
<li>yarn-site.xml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 分别指定RM的地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;slave1&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 指定ZK集群地址 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;master:2181,master2:2181,slave1:2181&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- Shuffle --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- RM中分配容器的内存最小值，默认1G --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;230&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- RM中分配容器的最大值，默认8G --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;700&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 可用物理内存大小，默认8G --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;700&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 虚拟内存检查是否开启 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><ol>
<li>拷贝一份配置文件模板，重命名为 <code>zoo.cfg</code><br><code>cd $ZK_HOME/conf;cp zoo_sample.cfg zoo.cfg</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190331095006564.png.ylit" alt></li>
<li><p>修改配置文件：<code>vi zoo.cfg</code><br>修改 <code>dataDir</code>，<code>dataLogDir</code>，添加 server 信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/home/hadoop/HA/data/zookeeper</span><br><span class="line">dataLogDir=/home/hadoop/HA/logs/zookeeper</span><br><span class="line">...</span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=master2:2888:3888</span><br><span class="line">server.3=slave1:2888:3888</span><br></pre></td></tr></table></figure>
<p><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190331112740321.png.ylit" alt></p>
</li>
<li>创建 <code>zoo.cfg</code> 中的 <code>dataDir</code>，在该目录下创建 <code>myid</code> 文件并添加内容，三个节点中的文件内容分别为 <code>1</code>，<code>2</code>，<code>3</code>，对应 <code>zoo.cfg</code> 中的 server.X。<br>如，<code>master</code> 节点中：<br><code>mkdir -p /home/hadoop/HA/data/zookeeper</code><br><code>echo 1 &gt; /home/hadoop/HA/data/zookeeper/myid</code></li>
</ol>
<h1 id="发送文件"><a href="#发送文件" class="headerlink" title="发送文件"></a>发送文件</h1><p>配置文件都已修改完毕，将需要的文件和目录都发送到其他节点上。</p>
<ul>
<li>Zookeeper（/usr/local/src 下）</li>
<li>Hadoop（/usr/local/src 下）</li>
<li>/home/hadoop/HA</li>
<li>/home/hadoop/.bash_profile（hadoop 用户的配置文件）</li>
</ul>
<p>如：<br>  <code>scp -r /usr/local/src/zookeeper-3.4.13 master2:/usr/local/src</code><br>  <code>scp /etc/profile slave1:/etc</code></p>
<h1 id="集群启动"><a href="#集群启动" class="headerlink" title="集群启动"></a>集群启动</h1><p><font color="#F4606C"><strong>需严格按照步骤执行。</strong></font><br>在 Hadoop 3 中，可用命令代替直接执行脚本文件：</p>
<ul>
<li><code>hadoop --workers --daemon</code> =&gt; <code>hadoop-daemons.sh</code></li>
<li><code>hadoop --daemon</code> =&gt; <code>hadoop-daemon.sh</code></li>
<li><code>hdfs --daemon</code> =&gt; <code>hadoop-daemon.sh</code></li>
<li>…</li>
</ul>
<ol>
<li>在所有节点中启动 JournalNode<br><code>hadoop-daemons.sh start journalnode</code>，注意是 <strong>s</strong> 脚本：<code>-daemons.sh</code>。<br><font color="#D6D5B7">Hadoop 3 可用：<code>hadoop --workers --daemon start journalnode</code></font><br><font color="#D6D5B7">* 单节点启动：<code>hadoop --daemon start journalnode</code></font><br><code>jps</code> 查看节点的 JVM 进程中是否有 JournalNode（没有的话应该是没启动成功，到 $HADOOP_HOME/logs 目录下看日志报什么错）：<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401103030740.png.ylit" alt></li>
<li>格式化 Active NameNode（master）<br><code>hdfs namenode -format</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401110608485.png.ylit" alt><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401110348261.png.ylit" alt></li>
<li>启动 NameNode 守护程序（NameNode Daemon）（Active NameNode 节点，master）<br><code>hadoop-daemon.sh start namenode</code>，注意是 <strong>非 s</strong> 脚本：<code>-daemon.sh</code>。<br><font color="#D6D5B7">Hadoop 3 可用：<code>hdfs --daemon start namenode</code></font><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402120041747.png.ylit" alt></li>
<li>在 Standby NameNode（master2）节点复制 Active NameNode（master）的元数据<br><code>hdfs namenode -bootstrapStandby</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402120340598.png.ylit" alt><br>成功的话应该能看到如下信息：<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402120248117.png.ylit" alt></li>
<li>在 Standby NameNode 节点启动 NameNode 守护程序（NameNode Daemon）<br><code>hadoop-daemon.sh start namenode</code></li>
<li>在每个节点启动 Zookeeper 服务<br><code>zkServer.sh start</code>（每个节点执行一次）<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401065812483.png.ylit" alt><br>正常情况下，Zookeeper 集群状态应该是由 <strong>一个 Leader</strong> 和 <strong>多个 Follower</strong> 组成。<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401073414225.png.ylit" alt="master"><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401073524653.png.ylit" alt="master2"><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401073558885.png.ylit" alt="slave1"><br><code>jps</code> 查看进程会有一个 <code>QuorumPeerMain</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190409035814029.png.ylit" alt="jps"></li>
<li><p>启动 DataNode 守护程序（DataNode Daemon）<br><code>hadoop-daemons.sh start datanode</code></p>
<font color="#D6D5B7">Hadoop 3 可用：<code>hdfs --workers --daemon start datanode</code></font></li>
<li>在 Active NameNode 上格式化 Zookeeper Failover Controller<br><code>hdfs zkfc -formatZK</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402031416731.png.ylit" alt></li>
<li>在 Active NameNode 上启动 DFS<br><code>start-dfs.sh</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402031454458.png.ylit" alt></li>
<li>查看两个 NameNode 节点状态<br><code>hdfs haadmin -getServiceState nn1</code><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402031454458.png.ylit" alt></li>
<li>通过浏览器检查每个 NameNode 的状态<br>\<ip>:&lt;端口&gt; 如：192.168.222.128:9870<br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402031804174.png.ylit" alt><br><img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190402031828294.png.ylit" alt></ip></li>
</ol>
<h1 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h1><h2 id="Zookeeper-启动了，查看状态却提示可能未启动"><a href="#Zookeeper-启动了，查看状态却提示可能未启动" class="headerlink" title="Zookeeper 启动了，查看状态却提示可能未启动"></a>Zookeeper 启动了，查看状态却提示可能未启动</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@master ~&gt; zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /usr/local/src/zookeeper-3.4.13/bin/../conf/zoo.cfg</span><br><span class="line">Error contacting service. It is probably not running.</span><br></pre></td></tr></table></figure>
<p>  <img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401072836002.png.ylit" alt><br>  看日志：<code>zookeeper.out</code>（这个日志文件会生成于当时执行 ZK 脚本所处的目录下）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hadoop@master ~&gt; tail -20 zookeeper.out</span><br><span class="line">        at java.net.Socket.connect(Socket.java:589)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:558)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:610)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:838)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:957)</span><br><span class="line">2019-04-01 19:23:36,364 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer$QuorumServer@184] - Resolved hostname: master2 to address: master2/192.168.222.129</span><br><span class="line">2019-04-01 19:23:36,365 [myid:1] - WARN  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@584] - Cannot open channel to 3 at election address slave1/192.168.222.130:3888</span><br><span class="line">java.net.ConnectException: 拒绝连接 (Connection refused)</span><br><span class="line">        at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)</span><br><span class="line">        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)</span><br><span class="line">        at java.net.Socket.connect(Socket.java:589)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:558)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:610)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:838)</span><br><span class="line">        at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:957)</span><br><span class="line">2019-04-01 19:23:36,366 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:QuorumPeer$QuorumServer@184] - Resolved hostname: slave1 to address: slave1/192.168.222.130</span><br><span class="line">2019-04-01 19:23:36,366 [myid:1] - INFO  [QuorumPeer[myid=1]/0:0:0:0:0:0:0:0:2181:FastLeaderElection@847] - Notification time out: 60000</span><br></pre></td></tr></table></figure></p>
<p>  关键词：拒绝连接。<br>  查看防火墙也都已关闭：<br>  <img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190401072537885.png.ylit" alt><br>  百度：<code>Cannot open channel to 3 at election address slave1/192.168.222.130:3888
java.net.ConnectException: 拒绝连接 (Connection refused)</code>，无果。<br>  百度：<code>Zookeeper 拒绝连接</code>，看到说是 <code>/etc/hosts</code> 文件的问题，需注释掉 <code>127.0.0.1</code> 行。一开始的 hosts 文件内容：<br>  <img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190406053843202.png.ylit" alt><br>  没按帖子说的直接注释，只删掉了每个 hosts 文件中该行末尾对应的主机名。重启 Zookeeper 后正常：<br>  <img src="http://pouotmqe7.bkt.clouddn.com/b/i/20190323/hhawqjm-20190406054019233.png.ylit" alt></p>
<h1 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h1><ul>
<li>2019-4-6<br>一顿操作：<br>开机；<br>启动全部 Zookeeper，正常；<br>启动 HDFS：<code>start-dfs.sh</code>，看似也正常；<br>NameNode 状态：<br>nn1 (master)：Standby<br>nn2 (master2)：Active<br>nn2 的 namenode 日志中有条警告，不知道有什么影响没；<br>其他日志均正常。<br><code>hadoop-hadoop-namenode-master2.log</code>：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  ...</span><br><span class="line">  2019-04-06 17:12:37,856 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted</span><br><span class="line">java.lang.InterruptedException: sleep interrupted</span><br><span class="line">	at java.lang.Thread.sleep(Native Method)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:469)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$300(EditLogTailer.java:399)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:416)</span><br><span class="line">	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:484)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:412)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="备用"><a href="#备用" class="headerlink" title="备用"></a>备用</h1><ul>
<li>nn1<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">2019-04-02 08:12:53,523 WARN org.apache.hadoop.hdfs.server.namenode.FSEditLog: Unable to determine input streams from QJM to [192.168.222.128:8485, 192.168.222.129:8485, 192.168.222.130:8485]. Skipping.</span><br><span class="line">org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:</span><br><span class="line">192.168.222.129:8485: Call From master/192.168.222.128 to master2:8485 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</span><br><span class="line">192.168.222.130:8485: Call From master/192.168.222.128 to slave1:8485 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</span><br><span class="line">192.168.222.128:8485: Call From master/192.168.222.128 to master:8485 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</span><br><span class="line">	at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81)</span><br><span class="line">	at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:286)</span><br><span class="line">	at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:142)</span><br><span class="line">	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.selectInputStreams(QuorumJournalManager.java:485)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.JournalSet.selectInputStreams(JournalSet.java:269)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1673)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1706)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1685)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:703)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:325)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1099)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:716)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:635)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:940)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:913)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1646)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1713)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>nn2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">2019-04-02 03:08:07,600 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn1, ipcAddress=master/192.168.222.128:8020, httpAddress=http://master:9870], try next.</span><br><span class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:88)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1954)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1442)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4716)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1293)</span><br><span class="line">	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:148)</span><br><span class="line">	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:14726)</span><br><span class="line">	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)</span><br><span class="line">	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)</span><br><span class="line">	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)</span><br><span class="line">	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)</span><br><span class="line">	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)</span><br><span class="line"></span><br><span class="line">	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.call(Client.java:1457)</span><br><span class="line">	at org.apache.hadoop.ipc.Client.call(Client.java:1367)</span><br><span class="line">	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)</span><br><span class="line">	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)</span><br><span class="line">	at com.sun.proxy.$Proxy16.rollEditLog(Unknown Source)</span><br><span class="line">	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:152)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:365)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:362)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:504)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Yout</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://youthug.github.io/blog/2019/03/30/HDFS-High-Availability-With-QJM/">https://youthug.github.io/blog/2019/03/30/HDFS-High-Availability-With-QJM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/blog/tags/Hadoop/">Hadoop</a><a class="post-meta__tags" href="/blog/tags/HDFS/">HDFS</a><a class="post-meta__tags" href="/blog/tags/HDFS-HA/">HDFS HA</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/blog/2019/03/25/Watermark-Images-in-Photoshop/"><span>用 Photoshop 批量给图片添加水印/LOGO</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(http://pouotmqe7.bkt.clouddn.com/8700af19ly1fvpabr30o6j22yo1o0q7i.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By Yout</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/blog/js/utils.js?version=1.6.1"></script><script src="/blog/js/fancybox.js?version=1.6.1"></script><script src="/blog/js/sidebar.js?version=1.6.1"></script><script src="/blog/js/copy.js?version=1.6.1"></script><script src="/blog/js/fireworks.js?version=1.6.1"></script><script src="/blog/js/transition.js?version=1.6.1"></script><script src="/blog/js/scroll.js?version=1.6.1"></script><script src="/blog/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>